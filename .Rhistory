# Option 1 for Cases - Load in live case data by county dataset (NYT)
library(readr)
url_case_data = ("https://raw.githubusercontent.com/nytimes/covid-19-data/master/live/us-counties.csv")
case_data <- read_csv(url(url_case_data))
# Only complete case data; row count is significantly reduced
case_data_complete <- case_data[complete.cases(case_data$fips), ]
row.names(case_data_complete) <- case_data_complete$fips
# Better - Option 2 for Cases - Load in cumulative case by county dataset (JHU)
url_case_data = ("https://raw.githubusercontent.com/govex/COVID-19/master/data_tables/JHU_USCountymap/df_Counties2020.csv")
case_data <- read_csv(url(url_case_data))
# Change date to most updated day
case_data <- case_data[case_data$dt == "2021-02-09",]
case_data <- case_data[!case_data$FIPS=="00000", ]
# select cool columns for us to analyze
case_data_select <- subset(case_data, select=c("FIPS", "ST_Name", "Confirmed", "Deaths", "Population", "IncidenceRate", "NewCases"))
# Only complete case data; row count is significantly reduced
case_data_complete <- case_data[complete.cases(case_data$FIPS), ]
# Remove accidentally duplicated entries (only 1)
case_data_complete <- unique(case_data_complete)
row.names(case_data_complete) <- case_data_complete$FIPS
# Load in mask survey by county dataset
url_mask_data = ("https://raw.githubusercontent.com/nytimes/covid-19-data/master/mask-use/mask-use-by-county.csv")
mask_data <- read_csv(url(url_mask_data))
row.names(mask_data) <- mask_data$COUNTYFP
# Combine case and mask datasets
case_and_mask_data <- merge(case_data_complete, mask_data, all=T, by='row.names')
# only complete cases
case_and_mask_data <- case_and_mask_data[complete.cases(case_and_mask_data), ]
row.names(case_and_mask_data) <- case_and_mask_data$FIPS
# Unemployment
# https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/
url_jobs_data = ("https://evancollins.com/Unemployment.csv")
jobs_data <- read_csv(url(url_jobs_data))
# select cool columns for us to analyze
jobs_data_select <- subset(jobs_data, select=c("fips_txt", "Unemployment_rate_2019", "Median_Household_Income_2019", "Civilian_labor_force_2019", "Med_HH_Income_Percent_of_State_Total_2019"))
jobs_data_select <- jobs_data_select[complete.cases(jobs_data_select),]
row.names(jobs_data_select) <- jobs_data_select$fips_txt
# Combine jobs with case and mask data
case_and_mask_and_jobs_data <- merge(case_and_mask_data, jobs_data_select, all=T, by='row.names')
case_and_mask_and_jobs_data <- case_and_mask_and_jobs_data[complete.cases(case_and_mask_and_jobs_data),]
row.names(case_and_mask_and_jobs_data) <- case_and_mask_and_jobs_data$FIPS
# Poverty Data
# https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/
url_poverty_data = ("https://evancollins.com/PovertyEstimates.csv")
poverty_data <- read_csv(url(url_poverty_data))
# select cool columns for us to analyze
poverty_data_select <- subset(poverty_data, select=c("FIPStxt", "PCTPOVALL_2019"))
poverty_data_select <- poverty_data_select[complete.cases(poverty_data_select),]
row.names(poverty_data_select) <- poverty_data_select$FIPStxt
# Combine poverty with case and mask and jobs data
case_and_mask_and_jobs_and_poverty_data <- merge(case_and_mask_and_jobs_data, poverty_data_select, all=T, by='row.names')
case_and_mask_and_jobs_and_poverty_data <- case_and_mask_and_jobs_and_poverty_data[complete.cases(case_and_mask_and_jobs_and_poverty_data),]
row.names(case_and_mask_and_jobs_and_poverty_data) <- case_and_mask_and_jobs_and_poverty_data$FIPS
# Education Data
# https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/
url_education_data = ("https://evancollins.com/Education.csv")
education_data <- read_csv(url(url_education_data))
# select cool columns for us to analyze
education_data_select <- subset(education_data, select=c("FIPS Code", "Percent of adults with less than a high school diploma, 2014-18", "Percent of adults with a bachelor's degree or higher, 2014-18"))
education_data_select <- education_data_select[complete.cases(education_data_select),]
row.names(education_data_select) <- education_data_select$`FIPS Code`
# Combine education with case and mask and jobs and poverty data
case_and_mask_and_jobs_and_poverty_and_education_data <- merge(case_and_mask_and_jobs_and_poverty_data, education_data_select, all=T, by='row.names')
case_and_mask_and_jobs_and_poverty_and_education_data <- case_and_mask_and_jobs_and_poverty_and_education_data[complete.cases(case_and_mask_and_jobs_and_poverty_and_education_data),]
row.names(case_and_mask_and_jobs_and_poverty_and_education_data) <- case_and_mask_and_jobs_and_poverty_and_education_data$FIPS
# Population Data
# https://www.ers.usda.gov/data-products/county-level-data-sets/download-data/
url_population_data = ("https://evancollins.com/PopulationEstimates.csv")
population_data <- read_csv(url(url_population_data))
# select cool columns for us to analyze
population_data_select <- subset(population_data, select=c("FIPStxt", "POP_ESTIMATE_2019", "R_NET_MIG_2019", "R_death_2019", "R_birth_2019", "Rural-urban_Continuum Code_2013"))
population_data_select <- population_data_select[complete.cases(population_data_select),]
row.names(population_data_select) <- population_data_select$FIPStxt
# Combine population with case and mask and jobs and poverty and education data
case_and_mask_and_jobs_and_poverty_and_education_and_population_data <- merge(case_and_mask_and_jobs_and_poverty_and_education_data, population_data_select, all=T, by='row.names')
case_and_mask_and_jobs_and_poverty_and_education_and_population_data <- case_and_mask_and_jobs_and_poverty_and_education_and_population_data[complete.cases(case_and_mask_and_jobs_and_poverty_and_education_and_population_data),]
row.names(case_and_mask_and_jobs_and_poverty_and_education_and_population_data) <- case_and_mask_and_jobs_and_poverty_and_education_and_population_data$FIPS
# Rough df
rough_df <- case_and_mask_and_jobs_and_poverty_and_education_and_population_data
# Select columns
rough_df <- subset(rough_df, select=c("Countyname", "ST_Name", "FIPS", "Confirmed", "Deaths", "IncidenceRate", "NewCases", "NEVER", "RARELY", "SOMETIMES", "FREQUENTLY", "ALWAYS", "Unemployment_rate_2019", "Median_Household_Income_2019", "Civilian_labor_force_2019", "Med_HH_Income_Percent_of_State_Total_2019", "PCTPOVALL_2019", "Percent of adults with less than a high school diploma, 2014-18", "Percent of adults with a bachelor's degree or higher, 2014-18", "POP_ESTIMATE_2019", "R_NET_MIG_2019", "R_death_2019", "R_birth_2019", "Rural-urban_Continuum Code_2013"))
names(rough_df)[names(rough_df) == "NewCases"] <- "NewCases_02_09_21"
names(rough_df)[names(rough_df) == "NEVER"] <- "never_mask_survey"
names(rough_df)[names(rough_df) == "RARELY"] <- "rarely_mask_survey"
names(rough_df)[names(rough_df) == "SOMETIMES"] <- "sometimes_mask_survey"
names(rough_df)[names(rough_df) == "FREQUENTLY"] <- "frequently_mask_survey"
names(rough_df)[names(rough_df) == "ALWAYS"] <- "always_mask_survey"
names(rough_df)[names(rough_df) == "Percent of adults with less than a high school diploma, 2014-18"] <- "percent_adults_less_than_hs"
names(rough_df)[names(rough_df) == "Percent of adults with a bachelor's degree or higher, 2014-18"] <- "percent_adults_bach_or_higher"
names(rough_df)[names(rough_df) == "Rural-urban_Continuum Code_2013"] <- "rural_urban_code"
# Save
covid_and_demographics <- rough_df
# Replace with your desired directory
write.csv(covid_and_demographics, "/Users/Evan/Desktop/s&ds 563/covid_and_demographics.csv")
View(rough_df)
dim(rough_df)
length(unique(rough_df$ST_Name))
install.packages(c("car", "corrplot", "FactoMineR", "PerformanceAnalytics"))
View(dat)
dat <- read_csv("/Users/kellyfarley/Desktop/Clingingsmith.csv")
library(tidyverse)
dat <- read_csv("/Users/kellyfarley/Desktop/Clingingsmith.csv")
View(dat)
names(dat)
View(dat)
install.packages(c("car", "corrplot", "FactoMineR", "PerformanceAnalytics"))
dat <- read_csv("/Users/kellyfarley/Desktop/Clingingsmith.csv")
View(dat)
range(dat$success)
names(dat)
?complete_ra
library(randomizr)
?complete_ra
length(dat)
dim(dat)[1]
library(randomizr)
library(tidyverse)
dat <- read_csv("/Users/kellyfarley/Desktop/Clingingsmith.csv")
set.seed(27)
# building null hypothesis: filling in the missing ones
dat <- dat %>%
mutate(Y1_star = views,
Y0_star = views)
# build null distro with a loop
sim <- 10000
ate_sim <- rep(NA, sims)
for(I in 1:sims){
dat <- data %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0]))
}
hist(ate_sim)
# checking against null distro: this will give pvalue
mean(abs(ate_sim) > abs(ate_est))
library(randomizr)
library(tidyverse)
dat <- read_csv("/Users/kellyfarley/Desktop/Clingingsmith.csv")
set.seed(27)
# building null hypothesis: filling in the missing ones
dat <- dat %>%
mutate(Y1_star = views,
Y0_star = views)
# build null distro with a loop
sim <- 10000
ate_sim <- rep(NA, sims)
for(I in 1:sim){
dat <- data %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0]))
}
hist(ate_sim)
# checking against null distro: this will give pvalue
mean(abs(ate_sim) > abs(ate_est))
library(randomizr)
library(tidyverse)
dat <- read_csv("/Users/kellyfarley/Desktop/Clingingsmith.csv")
set.seed(27)
# building null hypothesis: filling in the missing ones
dat <- dat %>%
mutate(Y1_star = views,
Y0_star = views)
# build null distro with a loop
sim <- 10000
ate_sim <- rep(NA, sim)
for(I in 1:sim){
dat <- data %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0]))
}
hist(ate_sim)
# checking against null distro: this will give pvalue
mean(abs(ate_sim) > abs(ate_est))
complete_ra(dim(dat)[1])
View(Dat)
View(dat)
for(i in 1:sim){
dat <- data %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0]))
}
or(i in 1:sim){
dat <- data %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0]))
}
for(i in 1:sim){
dat <- data %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0])))
}
for(i in 1:sim){
dat <- data %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0])))
}
for(i in 1:sim){
dat <- dat %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0])))
}
dat <- dat %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
View(dat)
for(i in 1:sim){
dat <- dat %>%
mutate(Z_sim = complete_ra(dim(dat)[1]),
Y_sim = Y1_star*Z_sim + (1-Z_sim)*Y0_star)
ate_sim[i] <- with(dat, mean(Y_sim[Z_sim==1]) - mean(Y_sim[Z_sim==0]))
}
View(dat)
ate_sim
hist(ate_sim)
mean(abs(ate_sim) > abs(ate_est))
names(dat)
ate_est <- with(dat, mean(views[success==1]) - mean(views[success==0]))
mean(abs(ate_sim) > abs(ate_est))
mean(abs(ate_sim) > abs(ate_est)))
mean(abs(ate_sim) > abs(ate_est))
mean(abs(ate_sim) > abs(ate_est))
mean(ate_sim > ate_est)
ate_est
install.packages(c("car", "corrplot", "FactoMineR", "PerformanceAnalytics"))
install.packages(c("car", "corrplot", "FactoMineR", "PerformanceAnalytics"))
library(corrplot)
install.packages("corrplot")
install.packages("PerformanceAnalytics")
install.packages("FactoMineR")
ok
install.packages("car")
install.packages("tidyverse")
