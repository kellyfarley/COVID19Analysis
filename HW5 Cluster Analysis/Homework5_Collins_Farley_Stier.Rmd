---
title: "S&DS 363 Homework 5"
author: "Evan Collins, Kelly Farley, Ken Stier"
date: "30 March 2021"
output:
  html_document:
   toc: yes
   toc_float:
     collapsed: no
  pdf_document:
    latex_engine : xelatex
---

```{r, echo=F, message=F, warning=F}
# clear env
rm(list=ls())

# Load packages
library(car)
library(tidyverse)
library(MASS)
library(biotools)
library(DiscriMiner)
library(klaR)
library(vegan)

#source("http://www.reuningscherer.net/Multivariate/R/CSQPlot.r.txt")
#The file wouldn't knit because the source link timed out, so I just copied the contents to run natively:
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
  #usually, vars is xxx$residuals or data from one group and label is for plot
  x<-cov(scale(vars),use="pairwise.complete.obs")
  squares<-sort(diag(as.matrix(scale(vars))%*%solve(x)%*%as.matrix(t(scale(vars)))))
  quantiles<-quantile(squares)
  hspr<-quantiles[4]-quantiles[2]
  cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
  degf<-dim(x)[1]
  quants<-qchisq(cumprob,df=degf)
  gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
  scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
  se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
  lower<-quants-2*se
  upper<-quants+2*se
  
  plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
       ylab="Squared MH Distance",main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
  lines(c(0,100),c(0,100),col=1)
  lines(quants,upper,col="blue",lty=2,lwd=2)
  lines(quants,lower,col="blue",lty=2,lwd=2)
  legend("topleft",c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
         pch=c(19,NA))
}
```

# Contributors

Evan Collins (evan.collins@yale.edu)

Kelly Farley (kelly.farley@yale.edu)

Ken Stier (ken.stier@yale.edu)

# The Dataset

*Raw dataset:*
COVID-19 infection and death statistics from U.S. counties (sourced from NYT), combined with economic, education, and population data (sourced from various government agencies) and also survey responses about mask-wearing frequencies (sourced from NYT). 3141 complete observations on 19 metric variables and 6 categorical variables. To avoid any outliers due to population size differences between counties, all variables are scaled as a percentage of population. Variable descriptions can be found [here](http://evancollins.com/variable_descriptions.html).

*Data of relevance for this pset:*

In previous psets, we had determined our favorite discriminating variables to be median household income, poverty rate, and confirmed COVID cases as a percentage of the population; we also determined that the log of each of these variables is more useful and normal than the raw variables, so we will be applying that transformation.

Should we choose to use these categorical variables for reference, the `rural_urban_code` and the `region` variables defined below tend to be great for grouping comparisons. The Rural-Urban Codes are numbered 1-9 according to descriptions provided by the USDA, and we consolidate them into: (1) "Urban" for codes 1-3, (2) "Suburban" for codes 4-6, and (3) "Rural" for codes 7-9. Region is determined for each county/state by the Census Bureau (Northeast, Midwest, South, and West).

```{r}
raw <- readr::read_csv("https://evancollins.com/covid_and_demographics.csv")

# create categorical variables: rural-urban code (3 levels), region (4 variables)
raw <- 
  raw %>%
    mutate(region = case_when(
      State_Name %in% c("Washington", "Oregon", "California", "Nevada", "Idaho", "Montana", "Utah", "Arizona", "Wyoming", "Colorado", "New Mexico", "Alaska", "Hawaii") ~ "West",
      State_Name %in% c("North Dakota", "South Dakota", "Nebraska", "Kansas", "Minnesota", "Iowa", "Missouri", "Wisconsin", "Illinois", "Michigan", "Indiana", "Ohio") ~ "Midwest",
      State_Name %in% c("Texas", "Oklahoma", "Arkansas", "Louisiana", "Mississippi", "Tennessee", "Kentucky", "Alabama", "Georgia", "Florida", "South Carolina", "North Carolina", "Virginia", "West Virginia", "District of Columbia", "Delaware", "Maryland") ~ "South",
      State_Name %in% c("Pennsylvania", "New Jersey", "Connecticut", "Rhode Island", "Massachusetts", "New Hampshire", "Vermont", "Maine", "New York") ~ "Northeast"),
      rural_urban_code = case_when(
        Rural_Urban_Code_2013 %in% c(1, 2, 3) ~ "Urban",
        Rural_Urban_Code_2013 %in% c(4, 5, 6) ~ "Suburban",
        Rural_Urban_Code_2013 %in% c(7, 8, 9) ~ "Rural")
      )
raw$rural_urban_code <- as.factor(raw$rural_urban_code) # Rural is reference

# log transformations of our continuous variables
raw$logMedian_Household_Income_2019 <- log(raw$Median_Household_Income_2019 + 0.0001)
raw$logPercent_Poverty_2019 <- log(raw$Percent_Poverty_2019 + 0.0001)
raw$logCovid_Confirmed_Cases_as_pct <- log(raw$Covid_Confirmed_Cases_as_pct + 0.0001)
```

# 1 Distance Metrics

*Think about what distance metrics are appropriate for your data based on data type. Write a few sentences about this.  Also think about whether you should standardize and/or transform your data (comment as appropriate).*

In our case, it probably makes the most sense to go with Euclidean distance, since we're working with a set of continuous variables without special relationships that might necessitate something like city distance (such as categorical or binary data). We might want to consider using one of the measures JDRS mentioned for ecological data (i.e., Sorenson or Jaccard), given that epidemiological data may bear similarity, but it isn't strictly called for.

Regarding standardization, that would almost definitely be a good idea. If any variables are scaled or distributed very differently, that difference will weight the greater/broader variables over the others. We might make an exception for groups of variables on standard scales (like a five-point scale) or if the differences are based on some intrinsic property of the variables (like if they're interrelated as in our previous pset), but those cases don't apply here. Let's throw together a very quick box plot just to see how our variables look:

```{r}
boxplot(raw$logCovid_Confirmed_Cases_as_pct, raw$logMedian_Household_Income_2019, raw$logPercent_Poverty_2019, names=c("covid cases", "household income", "poverty rate"))
```

Yeah, those variables are very far from each other and differently distributed too. Let's go ahead and standardize by subtracting the mean and dividing by standard deviation.

```{r}
raw$tr_log_covid <- (raw$logCovid_Confirmed_Cases_as_pct - mean(raw$logCovid_Confirmed_Cases_as_pct)) / sd(raw$logCovid_Confirmed_Cases_as_pct)
raw$tr_log_income <- (raw$logMedian_Household_Income_2019 - mean(raw$logMedian_Household_Income_2019)) / sd(raw$logMedian_Household_Income_2019)
raw$tr_log_poverty <- (raw$logPercent_Poverty_2019 - mean(raw$logPercent_Poverty_2019)) / sd(raw$logPercent_Poverty_2019)
boxplot(raw$tr_log_covid, raw$tr_log_income, raw$tr_log_poverty, names=c("covid cases", "household income", "poverty rate"))
```

It looks a lot better! But the distribution of COVID cases could fit the others better. It looks like the issue might be from outliers created by the log transformation. Let's try that one without.

```{r}
raw$tr_covid <- (raw$Covid_Confirmed_Cases_as_pct - mean(raw$Covid_Confirmed_Cases_as_pct)) / sd(raw$Covid_Confirmed_Cases_as_pct)
boxplot(raw$tr_covid, raw$tr_log_income, raw$tr_log_poverty, names=c("covid cases", "household income", "poverty rate"))
```

Gorgeous! All nicely standardized.

# 2 Hierarchical Cluster Analysis
Try various forms of hierarchical cluster analysis.  Try at least two different distance metrics and two agglomeration methods.  Produce dendrograms and comment on what you observe.

# 3 If possible, run the SAS macro or the R function to think about how many groups you want to retain (i.e. get the plots of cluster distance, R-squared, etc).  If you canâ€™t run this, discuss how many groups you think are present.  

# 4 Run k-means clustering on your data.  Compare results to what you got in 3.)  Include a sum of squares vs. k (number of clusters) plot and comment on how many groups exist.

# 5 Comment on the number of groups that seem to be present based on what you find above.

# 6 Write a few sentences describing your final groups.