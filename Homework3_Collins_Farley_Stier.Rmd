---
title: "S&DS 363 Homework 3"
author: "Evan Collins, Kelly Farley, Ken Stier"
date: "11 March 2021"
output:
  html_document:
   toc: yes
   toc_float:
     collapsed: no
  pdf_document:
    latex_engine : xelatex
---

```{r, echo=F, message=F, warning=F}
# Load packages
library(corrplot)
library(PerformanceAnalytics)
library(FactoMineR)
library(car)
library(tidyverse)
library(ggplot2)
library(Rmisc)
library(MASS)
library(biotools)
library(DiscriMiner)
library(klaR)
library(car)
```

# The Dataset

*Raw dataset:*
COVID-19 infection and death statistics from U.S. counties (sourced from NYT), combined with economic, education, and population data (sourced from various government agencies) and also survey responses about mask-wearing frequencies (sourced from NYT). 3141 complete observations on 19 metric variables and 6 categorical variables. To avoid any outliers due to population size differences between counties, all variables are scaled as a percentage of population. Variable descriptions can be found [here](http://evancollins.com/variable_descriptions.html).



*Dataset for this pset:*
Adapted from the dataset described above, the dataset used in this pset has the aggregated Rural-Urban code (`Rural_Urban_Code_2013`) as the categorical variable. The five continuous variables used are cumulative COVID-19 cases as percent of total county population (`Covid_Confirmed_Cases_as_pct`), median household income (`Median_Household_Income_2019`), unemployment rate (`Unemployment_Rate_2019`), percent poverty (`Percent_Poverty_2019`), and percent of adults with less than a high school degree (`Percent_Adults_Less_Than_HS`). The unique county identifier (`FIPS`) is also contained in the dataset.

Note that by the end of number 1, the dataset is simplified to include `Rural_Urban_Code_2013` as the categorical variable and three continuous variables: `logMedian_Household_Income_2019`, `logPercent_Poverty_2019`, and `logCovid_Confirmed_Cases_as_pct`.

The Rural-Urban Codes are numbered 1-9 according to the following descriptions provided by the USDA.

<center>
![](rural_urban_codes_description.png)
</center>

We will regroup codes 1 through 9 as into three groups: (1) "Urban" for codes 1-3, (2) "Suburban" for codes 4-6, and (3) "Rural" for codes 7-9.


```{r}
raw
url_data = ("https://evancollins.com/covid_and_demographics.csv")
raw <- read_csv(url(url_data))
raw <- as.data.frame(raw)
db <- subset(raw, select=c(4,20,10,11,13,14,22)) # include only pertinent columns
# Regroup Rural-Urban Codes
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 1] <- "Urban"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 2] <- "Urban"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 3] <- "Urban"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 4] <- "Suburban"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 5] <- "Suburban"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 6] <- "Suburban"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 7] <- "Rural"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 8] <- "Rural"
db$Rural_Urban_Code_2013[db$Rural_Urban_Code_2013 == 9] <- "Rural"
```

# 1 Evaluate assumptions

One of the first things is to compare the continuous variables between groups.

```{r}
boxplot(Covid_Confirmed_Cases_as_pct ~ Rural_Urban_Code_2013, data = db, horizontal = T, main = "Cumulative Percent COVID-19 Death by Rural-Urban Group")
```

```{r}
boxplot(Median_Household_Income_2019 ~ Rural_Urban_Code_2013, data = db, horizontal = T, main = "Median Household Incole by Rural-Urban Group")
```

```{r}
boxplot(Unemployment_Rate_2019 ~ Rural_Urban_Code_2013, data = db, horizontal = T, main = "Unemployment Rate by Rural-Urban Group")
```
```{r}
boxplot(Percent_Poverty_2019 ~ Rural_Urban_Code_2013, data = db, horizontal = T, main = "Percent Poverty by Rural-Urban Group")
```

```{r}
boxplot(Percent_Adults_Less_Than_HS ~ Rural_Urban_Code_2013, data = db, horizontal = T, main = "Percent Adults Less than HS Education by Rural-Urban Group")
```


Next, we should check the assumption of multivariate normality in each group. 

```{r}
#see if data is multivariate normal in EACH GROUP
#get online function
source("http://www.reuningscherer.net/multivariate/R/CSQPlot.r.txt")

#examine multivariate normality within each rural-urban group
# par(mfrow = c(1,3), pty = "s", cex = 0.8)
CSQPlot(db[db$Rural_Urban_Code_2013 == "Urban", 3:7], label = "Urban")
CSQPlot(db[db$Rural_Urban_Code_2013 == "Suburban", 3:7], label = "Suburban")
CSQPlot(db[db$Rural_Urban_Code_2013 == "Rural", 3:7], label = "Rural")
```

In general, most data points for each rural-urban group lie within the 95% confidence limits in the above plots. Taking log transforms and square root transforms for all continuous variables were attempted, but this resulted in even greater deviation from the 95% confidence limits. Let's try digging in deeper as to how to solve this lack of multivariate normality.

```{r}
db$sqrtUnemployment_Rate_2019 <- sqrt(db$Unemployment_Rate_2019)
db$sqrtMedian_Household_Income_2019 <- sqrt(db$Median_Household_Income_2019)
db$sqrtPercent_Poverty_2019 <- sqrt(db$Percent_Poverty_2019)
db$sqrtPercent_Adults_Less_Than_HS <- sqrt(db$Percent_Adults_Less_Than_HS)
db$sqrtCovid_Confirmed_Cases_as_pct <- sqrt(db$Covid_Confirmed_Cases_as_pct)

db$logUnemployment_Rate_2019 <- log(db$Unemployment_Rate_2019 + 0.0001)
db$logMedian_Household_Income_2019 <- log(db$Median_Household_Income_2019 + 0.0001)
db$logPercent_Poverty_2019 <- log(db$Percent_Poverty_2019 + 0.0001)
db$logPercent_Adults_Less_Than_HS <- log(db$Percent_Adults_Less_Than_HS + 0.0001)
db$logCovid_Confirmed_Cases_as_pct <- log(db$Covid_Confirmed_Cases_as_pct + 0.0001)

#examine multivariate normality within each rural-urban group
# sqrt
#CSQPlot(db[db$Rural_Urban_Code_2013 == "Urban", 8:12], label = "Urban")
#CSQPlot(db[db$Rural_Urban_Code_2013 == "Suburban", 8:12], label = "Suburban")
#CSQPlot(db[db$Rural_Urban_Code_2013 == "Rural", 8:12], label = "Rural")
 
# log
#CSQPlot(db[db$Rural_Urban_Code_2013 == "Urban", 13:17], label = "Urban")
#CSQPlot(db[db$Rural_Urban_Code_2013 == "Suburban", 13:17], label = "Suburban")
#CSQPlot(db[db$Rural_Urban_Code_2013 == "Rural", 13:17], label = "Rural")
```

We checked normal quantile plots for each log variable. All were rougly linear (and hence normal) with one notable exception: `logCovid_Confirmed_Cases_as_pct`. 

```{r}
#qqnorm(db$logUnemployment_Rate_2019)
#qqnorm(db$logMedian_Household_Income_2019)
#qqnorm(db$logPercent_Poverty_2019)
#qqnorm(db$logPercent_Adults_Less_Than_HS)
qqnorm(db$logCovid_Confirmed_Cases_as_pct)
```

Let's try omitting outliers. The outlier exclusion method is to exclude any `logCovid_Confirmed_Cases_as_pct` values that are more than 1.5 x IQR lower than our first quartile and more than than 1.5 x IQR above the third quartile. 

```{r}
db[which(db$logCovid_Confirmed_Cases_as_pct > quantile(db$logCovid_Confirmed_Cases_as_pct, .25) - 1.5*IQR(db$logCovid_Confirmed_Cases_as_pct) & db$logCovid_Confirmed_Cases_as_pct < quantile(db$logCovid_Confirmed_Cases_as_pct, .75) + 1.5*IQR(db$logCovid_Confirmed_Cases_as_pct)),] -> db_clean

# Report percentage of items this restriction included
(length(db$logCovid_Confirmed_Cases_as_pct) - length(db_clean$logCovid_Confirmed_Cases_as_pct)) / length(db$logCovid_Confirmed_Cases_as_pct)
```

This outlier exclusion method removed about 5% of items, which is somewhat high. The outlier exclusion method of 3 x IQR only removed 2% of items, but ultimately yielded minimal improvements to our chi-square quantile plots.

Now, with our COVID cases outlier-clean dataset, let's check the normal quantile plot of `logCovid_Confirmed_Cases_as_pct`.

```{r}
qqnorm(db_clean$logCovid_Confirmed_Cases_as_pct)
```

This assumes a much more normal pattern.

Now, with the COVID cases outlier-clean dataset, we should check the assumption of multivariate normality in each group. We will use the log-transformed continuous variables.

```{r}
#see if data is multivariate normal in EACH GROUP
#get online function
source("http://www.reuningscherer.net/multivariate/R/CSQPlot.r.txt")

#examine multivariate normality within each rural-urban group
# par(mfrow = c(1,3), pty = "s", cex = 0.8)
CSQPlot(db_clean[db_clean$Rural_Urban_Code_2013 == "Urban", 13:17], label = "Urban Counties")
CSQPlot(db_clean[db_clean$Rural_Urban_Code_2013 == "Suburban", 13:17], label = "Suburban Counties")
CSQPlot(db_clean[db_clean$Rural_Urban_Code_2013 == "Rural", 13:17], label = "Rural Counties")
```

Still, it seems that the data does not assume multivariate normality. 

Let's try applying the 1.5 x IQR outlier exclusion method to the other four log-transformed variables. Although their respective qqnorm plots above did not demonstrate clear issues, there were minor outliers that could be affecting multivariate normality.

```{r}
db_clean[which(db_clean$logUnemployment_Rate_2019 > quantile(db_clean$logUnemployment_Rate_2019, .25) - 1.5*IQR(db_clean$logUnemployment_Rate_2019) & db_clean$logUnemployment_Rate_2019 < quantile(db_clean$logUnemployment_Rate_2019, .75) + 1.5*IQR(db_clean$logUnemployment_Rate_2019)),] -> db_clean_all

db_clean_all[which(db_clean_all$logMedian_Household_Income_2019 > quantile(db_clean_all$logMedian_Household_Income_2019, .25) - 1.5*IQR(db_clean_all$logMedian_Household_Income_2019) & db_clean_all$logMedian_Household_Income_2019 < quantile(db_clean_all$logMedian_Household_Income_2019, .75) + 1.5*IQR(db_clean_all$logMedian_Household_Income_2019)),] -> db_clean_all

db_clean_all[which(db_clean_all$logPercent_Poverty_2019 > quantile(db_clean_all$logPercent_Poverty_2019, .25) - 1.5*IQR(db_clean_all$logPercent_Poverty_2019) & db_clean_all$logPercent_Poverty_2019 < quantile(db_clean_all$logPercent_Poverty_2019, .75) + 1.5*IQR(db_clean$logPercent_Poverty_2019)),] -> db_clean_all

db_clean_all[which(db_clean_all$logPercent_Adults_Less_Than_HS > quantile(db_clean_all$logPercent_Adults_Less_Than_HS, .25) - 1.5*IQR(db_clean_all$logPercent_Adults_Less_Than_HS) & db_clean_all$logPercent_Adults_Less_Than_HS < quantile(db_clean_all$logPercent_Adults_Less_Than_HS, .75) + 1.5*IQR(db_clean_all$logPercent_Adults_Less_Than_HS)),] -> db_clean_all
```

Now, with the all univariate outliers removed across all continuous variables, we should check the assumption of multivariate normality in each group. We will use the log-transformed continuous variables.

```{r}
#see if data is multivariate normal in EACH GROUP
#get online function
source("http://www.reuningscherer.net/multivariate/R/CSQPlot.r.txt")

#examine multivariate normality within each rural-urban group
# par(mfrow = c(1,3), pty = "s", cex = 0.8)
CSQPlot(db_clean_all[db_clean_all$Rural_Urban_Code_2013 == "Urban", 13:17], label = "Urban Counties")
CSQPlot(db_clean_all[db_clean_all$Rural_Urban_Code_2013 == "Suburban", 13:17], label = "Suburban Counties")
CSQPlot(db_clean_all[db_clean_all$Rural_Urban_Code_2013 == "Rural", 13:17], label = "Rural Counties")
```

The above transformations have enabled the data to assume a more multivariate distribution. However, more than 5% of data lies outside the 95% confidence limits. 

Let try to us reduce the number of continuous variable columns of our dataset that will enable better multivariate normality. 

```{r}
#see if data is multivariate normal in EACH GROUP
#get online function
source("http://www.reuningscherer.net/multivariate/R/CSQPlot.r.txt")

#examine multivariate normality within each rural-urban group
# par(mfrow = c(1,3), pty = "s", cex = 0.8)
CSQPlot(db_clean_all[db_clean_all$Rural_Urban_Code_2013 == "Urban", c(14,15,17)], label = "Urban Counties")
CSQPlot(db_clean_all[db_clean_all$Rural_Urban_Code_2013 == "Suburban", c(14,15,17)], label = "Suburban Counties")
CSQPlot(db_clean_all[db_clean_all$Rural_Urban_Code_2013 == "Rural", c(14,15,17)], label = "Rural Counties")
```

This conforms with multivariate normality within each group.

Hence, for the remainder of this pset, the dataset analyzed `db_final` will include rural urban code as the categorical variable and three continuous variables: `logMedian_Household_Income_2019`, `logPercent_Poverty_2019`, and `logCovid_Confirmed_Cases_as_pct`.

```{r}
db_final <- db_clean_all[, c(1,2,14,15,17)]

# Create column for rural urban code as number
# 1 = Urban, 2 = Suburban, 3 = Rural
db_final$Rural_Urban_Code_2013_Number <- db_final$Rural_Urban_Code_2013
db_final$Rural_Urban_Code_2013_Number[db_final$Rural_Urban_Code_2013_Number == "Urban"] <- 1
db_final$Rural_Urban_Code_2013_Number[db_final$Rural_Urban_Code_2013_Number == "Suburban"] <- 2
db_final$Rural_Urban_Code_2013_Number[db_final$Rural_Urban_Code_2013_Number == "Rural"] <- 3
db_final$Rural_Urban_Code_2013_Number <- as.numeric(db_final$Rural_Urban_Code_2013_Number)

head(db_final)
```


It’s helpful to view where the groups live relative to each other two variables at a time.

```{r}
#make matrix plot to look at differences between groups
plot(db_final[,3:5], col = db_final[,6]+3, pch = db_final[,6]+15, cex = 1.2)
```

It seems like the covariance 'footprints' are not the same between the urban/suburban/rural groups for these three continuous variables. 

```{r}
names(db_final)
```

```{r}
#visually compare sample covariance matrices
print("Covariance Matrix for Urban Counties")
cov(db_final[db_final$Rural_Urban_Code_2013=="Urban", 3:5])
```


```{r}
print("Covariance Matrix for Suburban Counties")
cov(db_final[db_final$Rural_Urban_Code_2013=="Suburban", 3:5])
```

```{r}
print("Covariance Matrix for Rural Counties")
cov(db_final[db_final$Rural_Urban_Code_2013=="Rural", 3:5])
```

```{r}
#compare standard deviations in each group
sumstats <- round(sqrt(aggregate(db_final[,3:5], by = list(db_final[,6]),FUN=var)),2)[,-1]
rownames(sumstats) <- c("Urban","Suburban", "Rural")
print("Standard Deviations by Group")
sumstats
```

```{r}
#calculate Box's M statistic
boxM(db_final[,3:5], db_final$Rural_Urban_Code_2013)
```

Wth a p-value significantly below the 0.05 threshold, it appears that the covariances matrices are **NOT** the same between groups, so we fit both linear DA and quadratic DA.


# 2 Discriminant Analysis

## Linear Discriminant Analysis

```{r}
#run linear discriminant analysis
county.disc <- lda(db_final[, 3:5], grouping = db_final$Rural_Urban_Code_2013)
summary(county.disc)
```

```{r}
#get univarite and multivariate comparisons
county.manova <- manova(as.matrix(db_final[, 3:5]) ~ db_final$Rural_Urban_Code_2013)
summary.manova(county.manova, test = "Wilks")
```

```{r}
summary.aov(county.manova)
```

Classification Results - note have to run lda separately with CV=TRUE option if you want both raw and cross-validated results.

```{r}
# raw results
(ctraw <- table(db_final$Rural_Urban_Code_2013, predict(county.disc)$class))
# total percent correct
round(sum(diag(prop.table(ctraw))), 2)
```


```{r}
#cross validated results
county.discCV <- lda(db_final[,3:5], grouping = db_final$Rural_Urban_Code_2013, CV = TRUE)
(ctCV <- table(db_final$Rural_Urban_Code_2013, county.discCV$class))
# total percent correct
round(sum(diag(prop.table(ctCV))),2)
```

### Need to ask about code box below; 

**for some reason county.disc$scaling is a 3x2 matrix; so forcing into one columns results in 6 rows, which can't be matrix multiplied by db_final[, 3:5]**

this is because the scaling array has LD1 and LD2 values (which is 2 total (#groups - 1))

i just considered the discriminant functions separately

```{r}
#get the scores - matrix product of original variables with DA coefficients
scores1 <- as.matrix(db_final[, 3:5])%*%matrix(c(county.disc$scaling[,1]), ncol = 1)

boxplot(scores1 ~ db_final$Rural_Urban_Code_2013, lwd = 3, col = c("red","blue", "green"), horizontal = T, main = "Rural-Urban Discriminant Scores by Function (LD1)", ylab = "Function")

#get the scores - matrix product of original variables with DA coefficients
scores2 <- as.matrix(db_final[, 3:5])%*%matrix(c(county.disc$scaling[,2]), ncol = 1)

boxplot(scores2 ~ db_final$Rural_Urban_Code_2013, lwd = 3, col = c("red","blue", "green"), horizontal = T, main = "Rural-Urban Discriminant Scores by Function (LD2)", ylab = "Function")
```

We can see that there is more evidence of differences between county type in the LD1 direction of discrimination.


## Quadratic Disciminant Analysis

```{r}
#run quadratic discriminant analysis
countyQ.disc <- qda(db_final[,3:5], grouping = db_final$Rural_Urban_Code_2013)
summary(countyQ.disc)
```

```{r}
# raw results - more accurate than using linear DA
ctrawQ <- table(db_final$Rural_Urban_Code_2013, predict(countyQ.disc)$class)
ctrawQ
```

```{r}
#cross validated results - better than CV for linear DA
county.discCVQ <- qda(db_final[,3:5], grouping = db_final$Rural_Urban_Code_2013, CV = TRUE)
ctCVQ <- table(db_final$Rural_Urban_Code_2013, county.discCVQ$class)
ctCVQ
# total percent correct
round(sum(diag(prop.table(ctCVQ))),2)
```

Quadratic discriminant analysis leads to correct classifications in 55% of instances. This is slightly better than linear discriminant analysis, which led to correct classifications in 54% of instances. 

## Stepwise Discriminant Analysis

From the T-test results above, we suspect that all three continuous variables are significant in this classification. Nevertheless, let's do stepwise discriminant analysis to verify their respective significances.

```{r}
#STEPWISE DA
#Here is leave-one out classification which is relatively stable - keeps only N
(step1 <- stepclass(Rural_Urban_Code_2013 ~  logMedian_Household_Income_2019 + logPercent_Poverty_2019 + logCovid_Confirmed_Cases_as_pct, data = db_final, method = "lda", direction = "both", fold = nrow(db_final)))
names(step1)
step1$result.pm
```


```{r}
#Do stepwise quadratic DA 
(step3 <- stepclass(Rural_Urban_Code_2013 ~ logMedian_Household_Income_2019 + logPercent_Poverty_2019 + logCovid_Confirmed_Cases_as_pct, data = db_final, method = "qda", direction = 'both', fold = nrow(db_final)))
```

For both stepwise LDA and QDA, the two significant variables are determined to be `logMedian_Household_Income_2019` and `logPercent_Poverty_2019`.

```{r}
#plot results in space spanned by chosen variables
#First, linear DA with N and GS - linear partition of space
partimat(as.factor(Rural_Urban_Code_2013) ~ logMedian_Household_Income_2019 + logPercent_Poverty_2019, data = db_final, method = "lda")

#Second, QDA - quadratic partition of space
partimat(as.factor(Rural_Urban_Code_2013) ~ logMedian_Household_Income_2019 + logPercent_Poverty_2019, data = db_final, method = "qda")

#Note that if all more than two variables, it does all pairs of variables - as expected, logMedian_Household_Income_2019 and logPercent_Poverty_2019 seem to be the best
partimat(as.factor(Rural_Urban_Code_2013) ~ logMedian_Household_Income_2019 + logPercent_Poverty_2019 + logCovid_Confirmed_Cases_as_pct, data = db_final, method = "qda")
```

Hence, stepwise discriminant analysis (both LDA and QDA) suggests to build a model with `logMedian_Household_Income_2019` and `logPercent_Poverty_2019` as the signifincant continuous variables. COVID-19 cases as percent of total county population is not as significant of a predictor than these other two variables. 


# 3 Wilk's Lambda Test

To determine if there is statistical evidence that the multivariate group means are different, use the MANOVA test.

```{r}
county.manova <- manova(as.matrix(db_final[, 3:5]) ~ db_final$Rural_Urban_Code_2013)
summary.manova(county.manova, test = "Wilks")
```

Wilk's lambda value is significant (p < 2.2e-16) so we conclude multivariate means are not the same.










